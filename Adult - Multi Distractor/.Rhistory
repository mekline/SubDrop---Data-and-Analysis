#Check who these jokers are!  Will want to make double sure they have not taken the survey twice...
mydata[is.na(mydata$HasWillow),]$Paycode
mydata[is.na(mydata$HasWillow),]$WorkerId
#################################################################
## Filter participants first for payment then for analysis
## We filter for:
## Completed a previous survey
## Completed fewer than 3/4 of trials
## Nonnative English Speaker, or not from USA
## Had a video problem
## And save WorkerID code lists for the future...
#################################################################
## Check through the data for compliance and multiple takers (the general full-set solution!!
## Use this for actually running the analysis!
#If your line doesn't have Turk, we'll give you a free pass on language, country, and video presentation
mydata[is.na(mydata$HasTurk),]$Answer.English <- "yes"
mydata[is.na(mydata$HasTurk),]$Answer.country <- "USA"
##Come up with a true subject labeling order
#mydata$willowSubNo <- as.numeric(as.character(mydata$willowSubNo))
#trueSub <- unique(mydata[,c('willowcode','willowSubNo', 'Paycode')])
#trueSub <- trueSub[order(trueSub$willowcode, trueSub$willowSubNo),]
#trueSub$trueSubNo <- 1:nrow(trueSub)
#
##And add it back to the dataframe
#mydata <- merge(mydata, trueSub, by=c('willowcode','willowSubNo', 'Paycode'))
#
##Now check if any Turk workerIDs have MORE THAN ONE subno
#participantNos <- aggregate(mydata$trueSubNo, by=list(mydata$WorkerId, mydata$trueSubNo), unique)
#participantNos <- participantNos[,1:2]
#names(participantNos) <- c('WorkerId','trueSubNo')
#
##Make sure it's in the correct order!!
#participantNos <- participantNos[order(participantNos$trueSubNo),]
#participantNos$isDup <- duplicated(participantNos$WorkerId)
#
##And merge back in
#mydata <- merge(mydata, participantNos, by=c('WorkerId','trueSubNo'), all.x=TRUE)
##((if you were willow-only, you pass this part!))
#mydata[is.na(mydata$HasTurk),]$isDup <- FALSE
#################################################################
## Recode and clean data & conditions
#Check for number of legal responses given in a single session!!
mydata[mydata$mistakeFlag == '',]$mistakeFlag <- 'bad input' #Fix up missing values
mydata$Error <- 0
mydata[mydata$mistakeFlag == 'bad input',]$Error <- 1
participant.responsecount <- aggregate(mydata$Error, by=list(mydata$Paycode), sum)
names(participant.responsecount) <- c("Paycode", "gotError")
participant.responsecount$LegalAnswers <- 6-participant.responsecount$gotError
mydata <- merge(mydata, participant.responsecount, by=c("Paycode"), all.x=TRUE)
###############################################
## Throw out people who took a Snazzy Potato task before!
#previousers <- read.csv('snazzy potato 8-19-2012.txt', header=F)
#names(previousers) <- c("WorkerId")
#thistime <- unique(mydata$WorkerId)
#previous <- intersect(previousers$WorkerId,thistime)
#
##Grr!  Go make sure to not pay those guys!....
#
#Save lists for next time (copy results into workerId txt files)
#unique(mydata$WorkerId)
#
#mydata$tookPreviously <- mydata$WorkerId %in% previous
#
#mydata <- mydata[mydata$tookPreviously == FALSE,]
#################################################################
## Drop for analysis
#(Remember, Willow-onliers got a free pass on the first 3 here...)
length(unique(mydata$Paycode))
#Print out worker IDs for Snazzy Potato list
unique(mydata$WorkerId)
mydata <- mydata[mydata$Answer.country == "USA" &
mydata$Answer.English == "yes" &
mydata$LegalAnswers > 4,] #You did at least 3/4 of the trials
#Throw out people who only had turk data (no code given...)
mydata <- mydata[!is.na(mydata$HasWillow),]
length(unique(mydata$Paycode)) #38
44-38
#################################################################
## Time to analyze the data!
# Read in the stimuli file
stimuli <- read.csv('stimuli_targets.csv', header=T)
mydata <- merge(mydata, stimuli, by=c('stimNo', 'verb', 'sentence'))
# Get answers ready for comparison (they were cleaned up by hand too!)
mydata$word1.clean <- toupper(mydata$word1.clean)
mydata$word2.clean <- toupper(mydata$word2.clean)
#Code whether each argument is included in the answer!
mydata$mentionSubject <- FALSE
mydata[mydata$word1.clean == mydata$Subject,]$mentionSubject <- TRUE
mydata[mydata$word2.clean == mydata$Subject,]$mentionSubject <- TRUE
mydata$mentionVerb <- FALSE
mydata[mydata$word1.clean == mydata$Verb,]$mentionVerb <- TRUE
mydata[mydata$word2.clean == mydata$Verb,]$mentionVerb <- TRUE
mydata$mentionObject <- FALSE
mydata[mydata$word1.clean == mydata$Object,]$mentionObject <- TRUE
mydata[mydata$word2.clean == mydata$Object,]$mentionObject <- TRUE
################################################
#Look at overall results
with(mydata, tapply(mentionSubject, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
with(mydata, tapply(mentionObject, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
with(mydata, tapply(mentionVerb, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
#Look at by-verb results (is anything item-specific?)
with(mydata, tapply(mentionSubject, list(trialVersion, verb), mean, na.rm=TRUE), drop=TRUE)
with(mydata, tapply(mentionObject, list(trialVersion, verb), mean, na.rm=TRUE), drop=TRUE)
########
#Lump by participant to get frequency scores!
mentionSubject <- aggregate(mydata$mentionSubject, by = list(mydata$Paycode, mydata$trialVersion), sum)
names(mentionSubject) <- c("Paycode", "trialVersion", "mentionSubject")
mentionObject <- aggregate(mydata$mentionObject, by = list(mydata$Paycode, mydata$trialVersion), sum)
names(mentionObject) <- c("Paycode", "trialVersion", "mentionObject")
mentionVerb <- aggregate(mydata$mentionVerb, by = list(mydata$Paycode, mydata$trialVersion), sum)
names(mentionVerb) <- c("Paycode", "trialVersion", "mentionVerb")
with(mentionSubject, tapply(mentionSubject, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
with(mentionObject, tapply(mentionObject, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
with(mentionVerb, tapply(mentionVerb, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
sum.na.rm <- function(x) { sum(x,na.rm=T) }
my.sd <- function(x) {sd(x)/sqrt(length(x))}
with(mentionSubject, tapply(mentionSubject, list(trialVersion), my.sd), drop=TRUE)
with(mentionObject, tapply(mentionObject, list(trialVersion), my.sd), drop=TRUE)
with(mentionVerb, tapply(mentionVerb, list(trialVersion), my.sd), drop=TRUE)
###STATISTICS
#MentionSubject?
#Note, trial version is within subject AND within item (eg WASH), so make the random slopes model accordingly
#subj_model <- lmer(mentionSubject ~ trialVersion + (trialVersion|Paycode) + (trialVersion|verb), data=mydata, family="binomial")
#Failed to converge! start removing slopes.
subj_model2 <- lmer(mentionSubject ~ trialVersion + (1|Paycode) + (trialVersion|verb), data=mydata, family="binomial")
summary(subj_model2)
#MentionObject?
#(Same model structure)
#obj_model <- lmer(mentionObject ~ trialVersion + (trialVersion|Paycode) + (trialVersion|verb), data=mydata, family="binomial")
#noconverge
obj_model2 <- lmer(mentionObject ~ trialVersion + (1|Paycode) + (trialVersion|verb), data=mydata, family="binomial")
summary(obj_model2)
mydata$ExactResp <- "Other"
mydata[mydata$word1.clean == mydata$Subject & mydata$word2.clean == mydata$Verb,]$ExactResp <- "SV"
mydata$ExactResp
mydata[mydata$word1.clean == mydata$Verb & mydata$word2.clean == mydata$Object,]$ExactResp <- "VO"
mydata$ExactResp
mydata[mydata$ExactResp == "Other",]$verbatim
names(mydata)
foo <- mydata[mydata$ExactResp == "Other",]
foo[,c("word1", "word2")]
mydata[mydata$word1.clean == mydata$Subject & mydata$word2.clean == mydata$Object,]$ExactResp <- "SO"
foo <- mydata[mydata$ExactResp == "Other",]
foo[,c("word1", "word2")]
foo[,c("word1.clean", "word2.clean")]
mydata[mydata$word1.clean == mydata$Object & mydata$word2.clean == mydata$Verb,]$ExactResp <- "OV"
foo <- mydata[mydata$ExactResp == "Other",]
foo[,c("word1.clean", "word2.clean")]
nrow(foo)
nrow(mydata)
46/227
onlydata <- mydata[mydata$ExactResponse == "SV" | mydata$ExactResponse == "VO", ]
subj_model <- lmer(mentionSubject ~ trialVersion + (trialVersion|Paycode) + (trialVersion|verb), data=onlydata, family="binomial")
onlydata
onlydata <- mydata[mydata$ExactResp == "SV" | mydata$ExactResp == "VO", ]
only_model <- lmer(mentionSubject ~ trialVersion + (trialVersion|Paycode) + (trialVersion|verb), data=onlydata, family="binomial")
only_model2 <- lmer(mentionSubject ~ trialVersion + (1|Paycode) + (trialVersion|verb), data=onlydata, family="binomial")
summary(only_model)
summary(subj_model2)
summary(obj_model2)
with(mydata, tapply(mentionSubject, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
with(mydata, tapply(mentionObject, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
mydata$ObOnly <- "Neither"
mydata[(mydata$word1.clean == mydata$Subject | mydata$word2.clean == mydata$Subject) & !(mydata$word1.clean == mydata$Object | mydata$word2.clean == mydata$Object)]$ObOnly <- "SubOnly"
mydata[(mydata$word1.clean == mydata$Subject | mydata$word2.clean == mydata$Subject) & !(mydata$word1.clean == mydata$Object | mydata$word2.clean == mydata$Object),]$ObOnly <- "SubOnly"
mydata$ObOnly
mydata[!(mydata$word1.clean == mydata$Subject | mydata$word2.clean == mydata$Subject) & (mydata$word1.clean == mydata$Object | mydata$word2.clean == mydata$Object),]$ObOnly <- "ObOnly"
mydata$ObOnly
diffdata <- mydata[mydata$ObOnly != "Neither",]
diffdata$wasOb <- diffdata$ObOnly == "ObOnly"
sum(diffdata$wasOb)
nrow(diffdata$wasOb)
nrow(diffdata)
120/202
binom.test(sum(diffdata$wasOb), nrow(diffdata))
mydata$wasN <- mydata$ObOnly == "Neither"
sum(mydata$wasN)
25/220
with(mentionSubject, tapply(mentionSubject, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
with(mentionObject, tapply(mentionObject, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
mydata$mentionObject
mentionOb_Agent.boot.mean = bootstrap(mydata[mydata$trialVersion=="agent",]$mentionObject, 1000, mean)
quantile(PersonFree.boot.mean$thetastar, c(0.025, 0.975))
mentionOb_Patient.boot.mean = bootstrap(mydata[mydata$trialVersion=="patient",]$mentionObject, 1000, mean)
quantile(PersonHand.boot.mean$thetastar, c(0.025, 0.975))
mentionSb_Agent.boot.mean = bootstrap(mydata[mydata$trialVersion=="agent",]$mentionSubject, 1000, mean)
quantile(PersonFree.boot.mean$thetastar, c(0.025, 0.975))
mentionSb_Patient.boot.mean = bootstrap(mydata[mydata$trialVersion=="patient",]$mentionSubject, 1000, mean)
quantile(PersonHand.boot.mean$thetastar, c(0.025, 0.975))
library(binom)
library(bootstrap)
mentionOb_Agent.boot.mean = bootstrap(mydata[mydata$trialVersion=="agent",]$mentionObject, 1000, mean)
quantile(PersonFree.boot.mean$thetastar, c(0.025, 0.975))
mentionOb_Patient.boot.mean = bootstrap(mydata[mydata$trialVersion=="patient",]$mentionObject, 1000, mean)
quantile(PersonHand.boot.mean$thetastar, c(0.025, 0.975))
mentionSb_Agent.boot.mean = bootstrap(mydata[mydata$trialVersion=="agent",]$mentionSubject, 1000, mean)
quantile(PersonFree.boot.mean$thetastar, c(0.025, 0.975))
mentionSb_Patient.boot.mean = bootstrap(mydata[mydata$trialVersion=="patient",]$mentionSubject, 1000, mean)
quantile(PersonHand.boot.mean$thetastar, c(0.025, 0.975))
mentionOb_Agent.boot.mean = bootstrap(mydata[mydata$trialVersion=="agent",]$mentionObject, 1000, mean)
quantile(mentionOb_Agent.boot.mean$thetastar, c(0.025, 0.975))
mentionOb_Patient.boot.mean = bootstrap(mydata[mydata$trialVersion=="patient",]$mentionObject, 1000, mean)
quantile(mentionOb_Patient.boot.mean$thetastar, c(0.025, 0.975))
mentionSb_Agent.boot.mean = bootstrap(mydata[mydata$trialVersion=="agent",]$mentionSubject, 1000, mean)
quantile(mentionSb_Agent.boot.mean$thetastar, c(0.025, 0.975))
mentionSb_Patient.boot.mean = bootstrap(mydata[mydata$trialVersion=="patient",]$mentionSubject, 1000, mean)
quantile(mentionSb_Patient.boot.mean$thetastar, c(0.025, 0.975))
with(mydata, tapply(mentionSubject, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
with(mydata, tapply(mentionObject, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
with(mydata, tapply(mentionVerb, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
with(mydata, tapply(mentionSubject, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
with(mydata, tapply(mentionObject, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
with(mydata, tapply(mentionVerb, list(trialVersion), mean, na.rm=TRUE), drop=TRUE)
mentionOb_Agent.boot.mean = bootstrap(mydata[mydata$trialVersion=="agent",]$mentionObject, 1000, mean)
quantile(mentionOb_Agent.boot.mean$thetastar, c(0.025, 0.975))
mentionOb_Patient.boot.mean = bootstrap(mydata[mydata$trialVersion=="patient",]$mentionObject, 1000, mean)
quantile(mentionOb_Patient.boot.mean$thetastar, c(0.025, 0.975))
mentionSb_Agent.boot.mean = bootstrap(mydata[mydata$trialVersion=="agent",]$mentionSubject, 1000, mean)
quantile(mentionSb_Agent.boot.mean$thetastar, c(0.025, 0.975))
mentionSb_Patient.boot.mean = bootstrap(mydata[mydata$trialVersion=="patient",]$mentionSubject, 1000, mean)
quantile(mentionSb_Patient.boot.mean$thetastar, c(0.025, 0.975))
mentionVb_Agent.boot.mean = bootstrap(mydata[mydata$trialVersion=="agent",]$mentionVerb, 1000, mean)
quantile(mentionSb_Agent.boot.mean$thetastar, c(0.025, 0.975))
mentionVb_Agent.boot.mean = bootstrap(mydata[mydata$trialVersion=="agent",]$mentionVerb, 1000, mean)
quantile(mentionVb_Agent.boot.mean$thetastar, c(0.025, 0.975))
mentionVb_Patient.boot.mean = bootstrap(mydata[mydata$trialVersion=="patient",]$mentionVerb, 1000, mean)
quantile(mentionVb_Patient.boot.mean$thetastar, c(0.025, 0.975))
setwd("~/Dropbox/_Projects/SubDrop - Adult Production/Experiment 2 - Two-five distractors/MD - with verb ratings")
###############################################################
## Library for tedlab: linguistic acceptability rating surveys on Mechanical Turk
## October 2009 Steve Piantadosi
## July, August, September 2010 Ted Gibson
###############################################################
library(languageR)
library(lme4)
library(stringr)
source("tedlab-misc.R")
mean.na.rm <- function(x) { mean(x,na.rm=T) }
turk.files <- list.files('batch')
willow.files <- list.files('log')
verb.files <- list.files('ratings')
###############################################################
## Read in the data files from turk
list.number <- 1;  # keep track of the list number
turkdata <- data.frame(NULL)
for(f in turk.files) {
tmp <- read.csv(paste('batch/',f, sep=''), header=T)
tmp$turkfile <- f
turkdata <- rbind(turkdata, tmp)
list.number <- list.number + 1
}
dropped.columns <- c("HITId", "HITTypeId","Title", "Description", "Keywords", "Reward", "CreationTime", "MaxAssignments", 	"RequesterAnnotation", "AssignmentDurationInSeconds", "AutoApprovalDelayInSeconds", "Expiration", "NumberOfSimilarHITs", 	"LifetimeInSeconds", "AssignmentId", "AcceptTime", "AutoApprovalTime", "ApprovalTime", "RejectionTime", 	"RequesterFeedback", "AssignmentStatus","LifetimeApprovalRate", "Last30DaysApprovalRate","Last7DaysApprovalRate" )
turkdata <- turkdata[, setdiff(names(turkdata), dropped.columns)]
###############################################################
## Read in the data files from willow
willowdata <- data.frame(NULL)
for (w in willow.files) {
tmp <- read.csv(paste('log/',w, sep=''), header=T)
tmp$willowfile <- w
tmp$willowcode <- paste(unlist(str_extract_all(w,"[0-9+]")),collapse='')
willowdata <- rbind(willowdata, tmp)
}
#Remove extra header lines...
willowdata <- willowdata[willowdata$paycode != "paycode",]
willowdata$Paycode <- willowdata$paycode
###############################################################
## Read in the data files from verb ratings!!
verbdata <- data.frame(NULL)
for (w in verb.files) {
tmp <- read.csv(paste('ratings/',w, sep=''), header=T)
tmp$verbfile <- w
tmp$verbcode <- paste(unlist(str_extract_all(w,"[0-9+]")),collapse='')
verbdata <- rbind(verbdata, tmp)
}
#Remove extra header lines...
verbdata <- verbdata[verbdata$paycode != "paycode",]
verbdata$Paycode <- verbdata$paycode
###############################################################
## Merge the information from turk and willow!
#How many do we start with?
length(unique(willowdata$Paycode)) #112
length(unique(verbdata$Paycode)) #102
length(unique(turkdata$WorkerId)) #100
#Save and standardize
turkdata$Paycode <- turkdata$Answer.payCode
turkdata$Answer.payCode <- NA
turkdata <- turkdata[,setdiff(names(turkdata), c("Answer.payCode"))]
turkdata$HasTurk <- TRUE
willowdata$HasWillow <- TRUE
mydata <- NULL
mydata <- merge(willowdata, turkdata, by=c("Paycode"), all.x=TRUE, all.y=TRUE)
mydata <- merge(mydata, verbdata, by=c("Paycode", "stimNo", "paycode", "verb"), all.x=TRUE, all.y=TRUE)
#If your line doesn't have Willow, you are a useless unmatched Turk entry!
#Check who these jokers are!  Will want to make double sure they have not taken the survey twice...
mydata[is.na(mydata$HasWillow),]$Paycode
mydata[is.na(mydata$HasWillow),]$WorkerId
#################################################################
## Filter participants first for payment then for analysis
## We filter for:
## Completed a previous survey
## Completed fewer than 3/4 of trials
## Nonnative English Speaker, or not from USA
## Had a video problem
## And save WorkerID code lists for the future...
#################################################################
## Check through the data for compliance and multiple takers (the general full-set solution!!
## Use this for actually running the analysis!
#If your line doesn't have Turk, we'll give you a free pass on language, country, and video presentation
mydata[is.na(mydata$HasTurk),]$Answer.English <- "yes"
mydata[is.na(mydata$HasTurk),]$Answer.country <- "USA"
##Come up with a true subject labeling order
#mydata$willowSubNo <- as.numeric(as.character(mydata$willowSubNo))
#trueSub <- unique(mydata[,c('willowcode','willowSubNo', 'Paycode')])
#trueSub <- trueSub[order(trueSub$willowcode, trueSub$willowSubNo),]
#trueSub$trueSubNo <- 1:nrow(trueSub)
#
##And add it back to the dataframe
#mydata <- merge(mydata, trueSub, by=c('willowcode','willowSubNo', 'Paycode'))
#
##Now check if any Turk workerIDs have MORE THAN ONE subno
#participantNos <- aggregate(mydata$trueSubNo, by=list(mydata$WorkerId, mydata$trueSubNo), unique)
#participantNos <- participantNos[,1:2]
#names(participantNos) <- c('WorkerId','trueSubNo')
#
##Make sure it's in the correct order!!
#participantNos <- participantNos[order(participantNos$trueSubNo),]
#participantNos$isDup <- duplicated(participantNos$WorkerId)
#
##And merge back in
#mydata <- merge(mydata, participantNos, by=c('WorkerId','trueSubNo'), all.x=TRUE)
##((if you were willow-only, you pass this part!))
#mydata[is.na(mydata$HasTurk),]$isDup <- FALSE
#################################################################
## Recode and clean data & conditions
#Check for number of legal responses given in a single session!!
mydata[is.na(mydata$mistakeFlag),]$mistakeFlag <- 'bad input'
mydata[mydata$mistakeFlag == '',]$mistakeFlag <- 'bad input' #Fix up missing values
mydata$Error <- 0
mydata[mydata$mistakeFlag == 'bad input',]$Error <- 1
participant.responsecount <- aggregate(mydata$Error, by=list(mydata$Paycode), sum)
names(participant.responsecount) <- c("Paycode", "gotError")
participant.responsecount$LegalAnswers <- 12-participant.responsecount$gotError
mydata <- merge(mydata, participant.responsecount, by=c("Paycode"), all.x=TRUE)
###############################################
## Throw out people who took a Snazzy Potato task before!
previousers <- read.csv('snazzy potato 11-20.txt', header=F)
names(previousers) <- c("WorkerId")
thistime <- unique(mydata$WorkerId)
previous <- intersect(previousers$WorkerId,thistime)
#
##Grr!  Go make sure to not pay those guys!....
#
#
mydata$tookPreviously <- mydata$WorkerId %in% previous
#
mydata <- mydata[mydata$tookPreviously == FALSE,]
#################################################################
## Drop for analysis
#(Remember, Willow-onliers got a free pass on the first 3 here...)
length(unique(mydata$Paycode))
#Print out worker IDs for Snazzy Potato list
unique(mydata$WorkerId)
mydata <- mydata[mydata$Answer.country == "USA" &
mydata$Answer.English == "yes" &
mydata$VideoProblem == "FALSE" &
mydata$LegalAnswers > 8,] #You did at least 3/4 of the trials
#Throw out people who only had turk data (no code given...)
mydata <- mydata[!is.na(mydata$HasWillow),]
length(unique(mydata$Paycode)) #91
112-91
setwd("~/Dropbox/_Projects/SubDrop/SubDrop - Adult Production/Experiment 2 - Two-five distractors/MD - with verb ratings")
###############################################################
## Library for tedlab: linguistic acceptability rating surveys on Mechanical Turk
## October 2009 Steve Piantadosi
## July, August, September 2010 Ted Gibson
###############################################################
library(languageR)
library(lme4)
library(stringr)
source("tedlab-misc.R")
mean.na.rm <- function(x) { mean(x,na.rm=T) }
turk.files <- list.files('batch')
willow.files <- list.files('log')
verb.files <- list.files('ratings')
###############################################################
## Read in the data files from turk
list.number <- 1;  # keep track of the list number
turkdata <- data.frame(NULL)
for(f in turk.files) {
tmp <- read.csv(paste('batch/',f, sep=''), header=T)
tmp$turkfile <- f
turkdata <- rbind(turkdata, tmp)
list.number <- list.number + 1
}
dropped.columns <- c("HITId", "HITTypeId","Title", "Description", "Keywords", "Reward", "CreationTime", "MaxAssignments", 	"RequesterAnnotation", "AssignmentDurationInSeconds", "AutoApprovalDelayInSeconds", "Expiration", "NumberOfSimilarHITs", 	"LifetimeInSeconds", "AssignmentId", "AcceptTime", "AutoApprovalTime", "ApprovalTime", "RejectionTime", 	"RequesterFeedback", "AssignmentStatus","LifetimeApprovalRate", "Last30DaysApprovalRate","Last7DaysApprovalRate" )
turkdata <- turkdata[, setdiff(names(turkdata), dropped.columns)]
###############################################################
## Read in the data files from willow
willowdata <- data.frame(NULL)
for (w in willow.files) {
tmp <- read.csv(paste('log/',w, sep=''), header=T)
tmp$willowfile <- w
tmp$willowcode <- paste(unlist(str_extract_all(w,"[0-9+]")),collapse='')
willowdata <- rbind(willowdata, tmp)
}
#Remove extra header lines...
willowdata <- willowdata[willowdata$paycode != "paycode",]
willowdata$Paycode <- willowdata$paycode
###############################################################
## Read in the data files from verb ratings!!
verbdata <- data.frame(NULL)
for (w in verb.files) {
tmp <- read.csv(paste('ratings/',w, sep=''), header=T)
tmp$verbfile <- w
tmp$verbcode <- paste(unlist(str_extract_all(w,"[0-9+]")),collapse='')
verbdata <- rbind(verbdata, tmp)
}
#Remove extra header lines...
verbdata <- verbdata[verbdata$paycode != "paycode",]
verbdata$Paycode <- verbdata$paycode
###############################################################
## Merge the information from turk and willow!
#How many do we start with?
length(unique(willowdata$Paycode)) #112
length(unique(verbdata$Paycode)) #102
length(unique(turkdata$WorkerId)) #100
#Save and standardize
turkdata$Paycode <- turkdata$Answer.payCode
turkdata$Answer.payCode <- NA
turkdata <- turkdata[,setdiff(names(turkdata), c("Answer.payCode"))]
turkdata$HasTurk <- TRUE
willowdata$HasWillow <- TRUE
mydata <- NULL
mydata <- merge(willowdata, turkdata, by=c("Paycode"), all.x=TRUE, all.y=TRUE)
mydata <- merge(mydata, verbdata, by=c("Paycode", "stimNo", "paycode", "verb"), all.x=TRUE, all.y=TRUE)
#If your line doesn't have Willow, you are a useless unmatched Turk entry!
#Check who these jokers are!  Will want to make double sure they have not taken the survey twice...
mydata[is.na(mydata$HasWillow),]$Paycode
mydata[is.na(mydata$HasWillow),]$WorkerId
#################################################################
## Filter participants first for payment then for analysis
## We filter for:
## Completed a previous survey
## Completed fewer than 3/4 of trials
## Nonnative English Speaker, or not from USA
## Had a video problem
## And save WorkerID code lists for the future...
#################################################################
## Check through the data for compliance and multiple takers (the general full-set solution!!
## Use this for actually running the analysis!
#If your line doesn't have Turk, we'll give you a free pass on language, country, and video presentation
mydata[is.na(mydata$HasTurk),]$Answer.English <- "yes"
mydata[is.na(mydata$HasTurk),]$Answer.country <- "USA"
##Come up with a true subject labeling order
#mydata$willowSubNo <- as.numeric(as.character(mydata$willowSubNo))
#trueSub <- unique(mydata[,c('willowcode','willowSubNo', 'Paycode')])
#trueSub <- trueSub[order(trueSub$willowcode, trueSub$willowSubNo),]
#trueSub$trueSubNo <- 1:nrow(trueSub)
#
##And add it back to the dataframe
#mydata <- merge(mydata, trueSub, by=c('willowcode','willowSubNo', 'Paycode'))
#
##Now check if any Turk workerIDs have MORE THAN ONE subno
#participantNos <- aggregate(mydata$trueSubNo, by=list(mydata$WorkerId, mydata$trueSubNo), unique)
#participantNos <- participantNos[,1:2]
#names(participantNos) <- c('WorkerId','trueSubNo')
#
##Make sure it's in the correct order!!
#participantNos <- participantNos[order(participantNos$trueSubNo),]
#participantNos$isDup <- duplicated(participantNos$WorkerId)
#
##And merge back in
#mydata <- merge(mydata, participantNos, by=c('WorkerId','trueSubNo'), all.x=TRUE)
##((if you were willow-only, you pass this part!))
#mydata[is.na(mydata$HasTurk),]$isDup <- FALSE
#################################################################
## Recode and clean data & conditions
#Check for number of legal responses given in a single session!!
mydata[is.na(mydata$mistakeFlag),]$mistakeFlag <- 'bad input'
mydata[mydata$mistakeFlag == '',]$mistakeFlag <- 'bad input' #Fix up missing values
mydata$Error <- 0
mydata[mydata$mistakeFlag == 'bad input',]$Error <- 1
participant.responsecount <- aggregate(mydata$Error, by=list(mydata$Paycode), sum)
names(participant.responsecount) <- c("Paycode", "gotError")
participant.responsecount$LegalAnswers <- 12-participant.responsecount$gotError
mydata <- merge(mydata, participant.responsecount, by=c("Paycode"), all.x=TRUE)
###############################################
## Throw out people who took a Snazzy Potato task before!
previousers <- read.csv('snazzy potato 11-20.txt', header=F)
names(previousers) <- c("WorkerId")
thistime <- unique(mydata$WorkerId)
previous <- intersect(previousers$WorkerId,thistime)
#
##Grr!  Go make sure to not pay those guys!....
#
#
mydata$tookPreviously <- mydata$WorkerId %in% previous
#
mydata <- mydata[mydata$tookPreviously == FALSE,]
#################################################################
## Drop for analysis
#(Remember, Willow-onliers got a free pass on the first 3 here...)
length(unique(mydata$Paycode))
#Print out worker IDs for Snazzy Potato list
unique(mydata$WorkerId)
mydata <- mydata[mydata$Answer.country == "USA" &
mydata$Answer.English == "yes" &
mydata$VideoProblem == "FALSE" &
mydata$LegalAnswers > 8,] #You did at least 3/4 of the trials
#Throw out people who only had turk data (no code given...)
mydata <- mydata[!is.na(mydata$HasWillow),]
length(unique(mydata$Paycode)) #91
